{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset\n\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom tqdm import tqdm\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"pycharm":{"name":"#%%\n"},"id":"IedLTuJ_caMf","execution":{"iopub.status.busy":"2022-02-19T08:08:46.900984Z","iopub.execute_input":"2022-02-19T08:08:46.902146Z","iopub.status.idle":"2022-02-19T08:08:47.569646Z","shell.execute_reply.started":"2022-02-19T08:08:46.902003Z","shell.execute_reply":"2022-02-19T08:08:47.568614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## set random seed","metadata":{"id":"2IZZK8L2caMh"}},{"cell_type":"code","source":"seed = 7\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"pycharm":{"name":"#%%\n"},"id":"CS1vAsg6caMi","execution":{"iopub.status.busy":"2022-02-19T08:08:48.553101Z","iopub.execute_input":"2022-02-19T08:08:48.553457Z","iopub.status.idle":"2022-02-19T08:08:48.56447Z","shell.execute_reply.started":"2022-02-19T08:08:48.553411Z","shell.execute_reply":"2022-02-19T08:08:48.563366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cycle GAN","metadata":{"id":"JWsSdAsVcaMj"}},{"cell_type":"markdown","source":"<img src=\"./../img/cycle_gan_arch.jpeg\">","metadata":{"id":"bfYi-8ZqcaMj"}},{"cell_type":"markdown","source":"in cycle gan, we have two generator and two discriminator.\n* one generator G_H_Z convert horse to zebra.\n* the other gen G_Z_H convert zebra to horse\n* and two discriminator D_Z, D_H to disc whether a zebra/horse is real or fake.\n\nthe generator take in a img of horse/zebra **A** of the shape (N, 3, 256, 256), then output a img **B** of zebra/horse of the same shape.\n\nthe disc's job is to indentify whether the horse/zebra is generated or not, it take a img (N, 3, 256, 256) as input, then output a patch (N, 1, 30, 30)\nwe use the output tensor and all ones/zeros tensor of the same shape to compute loss (MSE LOSS) to train disc.\n\nthe generator's loss is much complicated, it can be divived into three parts:\n1. first part is the same as disc'loss, put the origin img and generated img to get output, compute loss1 using the output and the all ones/zeros label (MSE LOSS).\n2. second part is: when we input a horse img into G_Z_H, since the G_Z_H is intended to generate horse img, we don not want it to change the input img, in other word, we wanna the output horse img of G_Z_H and input horse img as close as possible. we use the input and output img to compute loss (L1 LOSS).\n3. third part is: when we input a horse img into G_H_Z, getting a output, and then put output into G_Z_H to reget a horse img. we wanna the final output horse and the input horse to as close as possible. so we use (L1 LOSS) to compute the loss.","metadata":{"id":"NYK82bPucaMk"}},{"cell_type":"markdown","source":"<img src=\"./../img/cyclegan.ppm\">\n","metadata":{"id":"4B171oQhcaMl"}},{"cell_type":"markdown","source":"the discriminator of cycle gan has two feature:\nin DCGAN, the disc take a image as input and output a single scacler (N, ). cycle gan's disc also input a image, but output a square tensor (N, 1, 30, 30). the (1, 30, 30) tensor's value are in range (0, 1) the same as other gan.\n\npeople are assuming that: each number of the square tnesor indicate that: whether the specific part of the image is close to the target or not.","metadata":{"id":"1maw8iYEcaMl"}},{"cell_type":"markdown","source":"## define model","metadata":{"id":"C_fHBFNxcaMl"}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channel):\n        super(ResidualBlock, self).__init__()\n        # first sublayer have Relu, the second do not have\n        self.conv_block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channel, in_channel, (3,3)),\n            nn.InstanceNorm2d(in_channel),\n            nn.ReLU(),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channel, in_channel, (3,3)),\n            nn.InstanceNorm2d(in_channel)\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\n\nclass Gen(nn.Module):\n    def __init__(self, img_channel) -> None:\n        super(Gen, self).__init__()\n        # up and down block's: inchannel, outchannel, kernel_size, stride, padding\n        # in down block: the first two elements is inchannel, outchannel\n        # in up block: the first two elements is outchannel, inchannel\n\n                                # (N, 3, 256, 256)\n        self.config_up_down = [[img_channel, 64, 7, 1, 3],\n                                # (N, 9, 256, 256)\n                               [64, 128, 4, 2, 1],\n                                # (N, 18, 128, 128)\n                               [128, 256, 4, 2, 1]]\n                                # (N, 36, 64, 64)\n                                # in up block the size goes the opposite direction\n\n\n\n        # create down and up block\n        self.down_block = self._down_block()\n        self.up_block = self._up_block()\n\n        # create residual block\n        self.num_residual_block = 9\n        residual_block = []\n        for i in range(self.num_residual_block):\n          residual_block.append(ResidualBlock(256))\n        self.residual_block = nn.Sequential(*residual_block)\n\n        # final tanh\n        # mind final part use conv instead of conv_transpose\n        self.final_part = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, img_channel, (7,7)),\n            nn.Tanh()\n        )\n\n\n\n\n    def _conv_block(self, inchannel, outchannel, k_s, s, p):\n        return nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, k_s, s, p),\n            nn.InstanceNorm2d(outchannel),\n            nn.ReLU()\n        )\n\n    def _deconv_block(self, inchannel, outchannel, k_s, s, p):\n        return nn.Sequential(\n            nn.ConvTranspose2d(inchannel, outchannel, k_s, s, p),\n            nn.InstanceNorm2d(outchannel),\n            nn.ReLU()\n        )\n    def _down_block(self):\n        down_layers = []\n        for i in range(len(self.config_up_down)):\n            down_layers.append(self._conv_block(self.config_up_down[i][0],\n                                                self.config_up_down[i][1],\n                                                self.config_up_down[i][2],\n                                                self.config_up_down[i][3],\n                                                self.config_up_down[i][4]))\n\n        return nn.Sequential(*down_layers)\n\n\n\n    def _up_block(self):\n        up_layers = []\n        # mind that there does not use:\n        #   conv(64 --> 3)\n        for i in range(len(self.config_up_down) - 1, 0, -1):\n            up_layers.append(self._deconv_block(self.config_up_down[i][1],\n                                                self.config_up_down[i][0],\n                                                self.config_up_down[i][2],\n                                                self.config_up_down[i][3],\n                                                self.config_up_down[i][4]))\n        return nn.Sequential(*up_layers)\n\n    def forward(self, x):\n        for layer in self.down_block:\n            x = layer(x)\n        for layer in self.residual_block:\n            x = layer(x)\n        for layer in self.up_block:\n            x = layer(x)\n        for layer in self.final_part:\n            x = layer(x)\n        return x","metadata":{"pycharm":{"name":"#%%\n"},"id":"7IlQ9HF3caMm","execution":{"iopub.status.busy":"2022-02-19T08:08:52.335512Z","iopub.execute_input":"2022-02-19T08:08:52.335812Z","iopub.status.idle":"2022-02-19T08:08:52.360991Z","shell.execute_reply.started":"2022-02-19T08:08:52.335781Z","shell.execute_reply":"2022-02-19T08:08:52.357021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the generator of cycle gan is made up of 3 parts:\n1. the first part is down block, decreasing the image size by conv.\n2. then use residual block while mataining the image size.\n3. finally the up block, to get the origin image size back.\n\nTIPS: the residual connection here is just elementwise add, we do not need do conv to obtain the same channel_num or image size.\n","metadata":{"id":"fd4coBlwcaMn"}},{"cell_type":"code","source":"class Disc(nn.Module):\n    \"\"\"\n    input: (N, 3, 256, 256)\n    output: (N, 1, 30, 30)\n    \"\"\"\n\n    def __init__(self, img_channel) -> None:\n        super(Disc, self).__init__()\n        # conv block's: inchannel, outchannel, kernel_size, stride, padding\n        # input: (3, 256, 256)\n        self.config_init = [img_channel, 64, 4, 2, 1]\n        # (64, 128, 128)\n        self.config_lst = [[64, 128, 4, 2, 1],\n                           # (128, 64, 64)\n                           [128, 256, 4, 2, 1],\n                           # (256, 32, 32)\n                           [256, 512, 4, 1, 1]]\n                           # (512, 31, 31)\n        self.config_final = [512, 1, 4, 1, 1]\n        # (1, 30, 30)\n        self.conv_layers = self._create_conv_layers()\n        self.sigmoid = nn.Sigmoid()\n\n    def _create_conv_layers(self):\n        layers_lst = []\n        # first layer don not have instance norm\n        layers_lst.append(self._conv_block(self.config_init, norm=False))\n\n        # intermediate layers follow the struct: conv, norm, leakyrelu\n        for i in range(len(self.config_lst)):\n            layers_lst.append(self._conv_block(self.config_lst[i], norm=True))\n        # final layer is just a conv\n        inchannel, outchannel, k_s, s, p = self.config_final[0],  self.config_final[1],  self.config_final[2],  self.config_final[3], self.config_final[4]\n        layers_lst.append(nn.Conv2d(inchannel, outchannel, k_s, s, p))\n\n        return nn.Sequential(*layers_lst)\n\n\n\n\n    def _conv_block(self, config, norm):\n        inchannel, outchannel, k_s, s, p = config[0],  config[1],  config[2],  config[3], config[4]\n        return nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, k_s, s, p),\n            nn.InstanceNorm2d(outchannel) if norm == True else nn.Identity(),\n            nn.LeakyReLU(0.2)\n\n        )\n    def forward(self, x):\n        for layer in self.conv_layers:\n            x = layer(x)\n        x = self.sigmoid(x)\n        return x","metadata":{"pycharm":{"name":"#%%\n"},"id":"pAn2hl5tcaMn","execution":{"iopub.status.busy":"2022-02-19T08:08:53.65664Z","iopub.execute_input":"2022-02-19T08:08:53.657507Z","iopub.status.idle":"2022-02-19T08:08:53.672163Z","shell.execute_reply.started":"2022-02-19T08:08:53.657466Z","shell.execute_reply":"2022-02-19T08:08:53.671112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define some params","metadata":{"id":"CrFthf0ocaMp"}},{"cell_type":"code","source":"LEARNING_RATE_GEN = 2e-4\nLEARNING_RATE_DISC = 2e-4\nLAMBDA_CYCLE = 10\nLAMBDA_INDENTITY = 5\nIMAGE_CHANNEL = 3\nNUM_EPOCH = 10\nBATCH_SIZE = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"pycharm":{"name":"#%%\n"},"id":"CzkfD5QpcaMp","execution":{"iopub.status.busy":"2022-02-19T08:08:58.46794Z","iopub.execute_input":"2022-02-19T08:08:58.470061Z","iopub.status.idle":"2022-02-19T08:08:58.513659Z","shell.execute_reply.started":"2022-02-19T08:08:58.470013Z","shell.execute_reply":"2022-02-19T08:08:58.512297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prepare data","metadata":{}},{"cell_type":"code","source":"# download and unzip dataset\n! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip && unzip horse2zebra.zip","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class horse_zebra(Dataset):\n    def __init__(self, transform, path_horse=\"./horse2zebra/trainA/\",path_zebra=\"./horse2zebra/trainB/\"):\n        self.path_horse = path_horse\n        self.path_zebra = path_zebra\n\n        self.transform = transform\n\n        self.lst_horse = os.listdir(self.path_horse)\n        self.lst_zebra = os.listdir(self.path_zebra)\n\n        # one thing about the dataset:\n        # the num of horse pic != num of zebra pic\n        self.length = max(len(self.lst_horse), len(self.lst_zebra))\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        # some pic on the dataset just have one channel\n        # so need to use function: convert(\"RGB\")\n        horse = Image.open(self.path_horse + self.lst_horse[torch.randint(len(self.lst_horse), size=(1,))]).convert(\"RGB\")\n        zebra = Image.open(self.path_zebra + self.lst_zebra[idx]).convert(\"RGB\")\n        if self.transform:\n            horse = self.transform(horse)\n            zebra = self.transform(zebra)\n        return horse, zebra\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:09:02.520846Z","iopub.execute_input":"2022-02-19T08:09:02.521745Z","iopub.status.idle":"2022-02-19T08:09:02.533084Z","shell.execute_reply.started":"2022-02-19T08:09:02.521689Z","shell.execute_reply":"2022-02-19T08:09:02.531959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = [0.5, 0.5, 0.5]\nstd = [0.5, 0.5, 0.5]\ntransform = transforms.Compose([\n    transforms.Resize((256,256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:09:03.417485Z","iopub.execute_input":"2022-02-19T08:09:03.418096Z","iopub.status.idle":"2022-02-19T08:09:03.425579Z","shell.execute_reply.started":"2022-02-19T08:09:03.41806Z","shell.execute_reply":"2022-02-19T08:09:03.424326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = horse_zebra(transform)\ndataloader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:09:03.895461Z","iopub.execute_input":"2022-02-19T08:09:03.89652Z","iopub.status.idle":"2022-02-19T08:09:03.905109Z","shell.execute_reply.started":"2022-02-19T08:09:03.89647Z","shell.execute_reply":"2022-02-19T08:09:03.903964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test samples\n# here, i choose to pick images in training set to visiualize whether the generator is learning\nfix_horse, fix_zebra = next(iter(dataloader))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:09:04.851183Z","iopub.execute_input":"2022-02-19T08:09:04.851731Z","iopub.status.idle":"2022-02-19T08:09:04.898928Z","shell.execute_reply.started":"2022-02-19T08:09:04.851685Z","shell.execute_reply":"2022-02-19T08:09:04.897937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## keep the image buffer\n\nin the paper, they do not put the image newly generated by Gen into Disc, instead, they keep a buffer of generated image and then sample image from the buffer to Disc.","metadata":{}},{"cell_type":"code","source":"class ImageBuffer:\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.data = []\n\n    def get_image(self, data):\n        # data is what Gen generated and send in func\n        # we will return a tensor of the same shape as data send in\n        tensor_return = []\n        for each in data:\n            each = each.unsqueeze(0)\n            if len(self.data) < self.max_size:\n                self.data.append(each)\n                tensor_return.append(each)\n            else:\n                # with prob == 0.5\n                # we will update the buffer\n                if torch.randn(1) < 0.5:\n                    choose = torch.randint(self.max_size, size=(1,))\n                    tensor_return.append(self.data[choose])\n                    self.data[choose] = each\n                else:\n                    tensor_return.append(each)\n        return torch.cat(tensor_return, dim=0)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:15:40.946227Z","iopub.execute_input":"2022-02-19T08:15:40.946534Z","iopub.status.idle":"2022-02-19T08:15:40.956287Z","shell.execute_reply.started":"2022-02-19T08:15:40.946503Z","shell.execute_reply":"2022-02-19T08:15:40.954836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define optim and loss","metadata":{}},{"cell_type":"code","source":"def weight_init(m):\n    \"\"\"\n    init model's prams, if a layer is a conv layer: set the weight to N(0, 0.02), set the bias to constant 0.\n    if a layer is a batch norm: set the weight to N(1, 0.02), set the bias to constant 0.\n    usage: model.apply(weight_init)\n\n    params:\n        m: model's layer\n    return:\n        None\n    \"\"\"\n    class_name = m.__class__.__name__\n    if class_name.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0)\n    if class_name.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:09:09.064423Z","iopub.execute_input":"2022-02-19T08:09:09.065595Z","iopub.status.idle":"2022-02-19T08:09:09.073858Z","shell.execute_reply.started":"2022-02-19T08:09:09.065545Z","shell.execute_reply":"2022-02-19T08:09:09.072747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G_H_Z = Gen(IMAGE_CHANNEL).to(DEVICE)\nG_Z_H = Gen(IMAGE_CHANNEL).to(DEVICE)\n\nD_H = Disc(IMAGE_CHANNEL).to(DEVICE)\nD_Z = Disc(IMAGE_CHANNEL).to(DEVICE)\n\nG_H_Z.apply(weight_init)\nG_Z_H.apply(weight_init)\nD_H.apply(weight_init)\nD_Z.apply(weight_init)\n\n# optim two networks' params at the same time\nopt_gen = optim.Adam(list(G_Z_H.parameters()) + list(G_H_Z.parameters()), lr=LEARNING_RATE_GEN, betas=(0.5, 0.999))\nopt_d_h = optim.Adam(D_H.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\nopt_d_z = optim.Adam(D_Z.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n\ngan_criterion = nn.MSELoss()\ncycle_criterion = nn.L1Loss()\nidentity_criterion = nn.L1Loss()\n\nmodel_checkpoint = {\n    'G_H_Z':None,\n    'G_Z_H':None,\n    'D_Z':None,\n    'D_H':None\n}\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"OQtymYlfcaMs","execution":{"iopub.status.busy":"2022-02-19T08:09:10.294817Z","iopub.execute_input":"2022-02-19T08:09:10.295504Z","iopub.status.idle":"2022-02-19T08:09:12.86786Z","shell.execute_reply.started":"2022-02-19T08:09:10.295467Z","shell.execute_reply":"2022-02-19T08:09:12.866703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## set up the training loop","metadata":{"id":"WY-a1xwqcaMt"}},{"cell_type":"markdown","source":"","metadata":{"pycharm":{"name":"#%% md\n"},"id":"zsfXD73McaMt"}},{"cell_type":"code","source":"! mkdir ./horse_output && mkdir ./zebra_output","metadata":{"execution":{"iopub.status.busy":"2022-02-19T08:34:23.243523Z","iopub.execute_input":"2022-02-19T08:34:23.243924Z","iopub.status.idle":"2022-02-19T08:34:24.042919Z","shell.execute_reply.started":"2022-02-19T08:34:23.24386Z","shell.execute_reply":"2022-02-19T08:34:24.041656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to sample images\ndef sample_image(fix_horse, fix_zebra):\n    fake_zebra = G_H_Z(fix_horse)\n    fake_horse = G_Z_H(fix_zebra)\n    cycle_horse = G_Z_H(fake_zebra)\n    cycle_zebra = G_H_Z(fake_horse)\n\n    grid_real_horse = torchvision.utils.make_grid(fix_horse, nrow=1, normalize=True)\n    grid_fake_zebra = torchvision.utils.make_grid(fake_zebra, nrow=1, normalize=True)\n    grid_cycle_horse = torchvision.utils.make_grid(cycle_horse, nrow=1, normalize=True)\n    grid_horse = torch.cat((grid_real_horse, grid_fake_zebra, grid_cycle_horse), dim=2)\n\n    grid_real_zebra = torchvision.utils.make_grid(fix_zebra, nrow=1, normalize=True)\n    grid_fake_horse = torchvision.utils.make_grid(fake_horse, nrow=1, normalize=True)\n    grid_cycle_zebra = torchvision.utils.make_grid(cycle_zebra, nrow=1, normalize=True)\n    grid_zebra = torch.cat((grid_real_zebra, grid_fake_horse, grid_cycle_zebra), dim=2)\n    torchvision.utils.save_image(grid_horse, f\"./horse_output/horse_epoch{epoch}_batch_{batch_idx}.png\", normalize=False)\n    torchvision.utils.save_image(grid_zebra, f\"./zebra_output/zebra_epoch{epoch}_batch_{batch_idx}.png\", normalize=False)\n    print(\"saving samples... done!\")\ndef save_checkpoint(dict, path=\"model.pth.tar\"):\n    \"\"\"\n    save the model and optim through a dictionary\n     {\"model\": model.state_dict(), \"optim\": optim.state_dict()} (*)\n\n\n    params:\n        data: a dict of the structure (*) storing the model and optim\n\n    return:\n        None\n    \"\"\"\n    now = time.strftime(\"%D_%H:%M\")\n    print(f\"saving checkpoint at {now}, path is '{path}'\")\n    torch.save(dict, path)\n    print(\"saving model... done!\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:43:23.731991Z","iopub.execute_input":"2022-02-19T08:43:23.732555Z","iopub.status.idle":"2022-02-19T08:43:23.744112Z","shell.execute_reply.started":"2022-02-19T08:43:23.732504Z","shell.execute_reply":"2022-02-19T08:43:23.742744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_zebra_buffer = ImageBuffer()\nfake_horse_buffer = ImageBuffer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(NUM_EPOCH):\n    loop = tqdm(dataloader, leave=True)\n    for batch_idx, (horse, zebra) in enumerate(loop):\n        G_H_Z.train()\n        G_Z_H.train()\n\n        horse = horse.to(DEVICE)\n        zebra = zebra.to(DEVICE)\n\n        #### train Gen ####\n        opt_gen.zero_grad()\n\n        # train generator with the lastest generated image\n        fake_horse = G_Z_H(zebra)\n        d_h_fake = D_H(fake_horse)\n        fake_zebra = G_H_Z(horse)\n        d_z_fake = D_Z(fake_zebra)\n        # gan loss\n        gan_loss = gan_criterion(d_z_fake, torch.ones_like((d_z_fake)).to(DEVICE)) + gan_criterion(d_h_fake, torch.ones_like(d_h_fake))\n\n        # identity loss\n        identity_zebra = G_H_Z(zebra)\n        identity_horse = G_Z_H(horse)\n        identity_loss = (identity_criterion(identity_zebra, zebra) + identity_criterion(identity_horse, horse)) * LAMBDA_INDENTITY\n\n        # cycle loss\n        cycle_horse = G_Z_H(fake_zebra)\n        cycle_zebra = G_H_Z(fake_horse)\n        cycle_loss = (cycle_criterion(cycle_horse, horse) + cycle_criterion(cycle_zebra, zebra)) * LAMBDA_CYCLE\n\n        Gen_loss = gan_loss + identity_loss + cycle_loss\n        Gen_loss.backward()\n        opt_gen.step()\n\n        #### train discriminator ####\n        opt_d_h.zero_grad()\n        # train discriminator with the image from buffer\n\n        d_h_real = D_H(horse)\n        # get image from buffer\n        fake_horse_ = fake_horse_buffer.get_image(fake_horse)\n        d_h_fake = D_H(fake_horse_.detach())\n        d_h_loss = gan_criterion(d_h_real, torch.ones_like(d_h_real).to(DEVICE)) + \\\n            gan_criterion(d_h_fake, torch.zeros_like(d_h_fake).to(DEVICE))\n        d_h_loss.backward()\n        opt_d_h.step()\n\n        opt_d_z.zero_grad()\n        d_z_real = D_Z(zebra)\n        fake_zebra_ = fake_zebra_buffer.get_image(fake_zebra)\n        d_z_fake = D_Z(fake_zebra_.detach())\n        d_z_loss = gan_criterion(d_z_real, torch.ones_like((d_z_real).to(DEVICE))) + \\\n            gan_criterion(d_z_fake, torch.zeros_like(d_z_fake).to(DEVICE))\n        d_z_loss.backward(retain_graph=True)\n        opt_d_z.step()\n\n        if batch_idx % 40 == 0:\n          G_Z_H.eval()\n          G_H_Z.eval()\n          with torch.no_grad():\n                # sample gans output\n                sample_image(fix_horse.to(DEVICE), fix_zebra.to(DEVICE))\n\n    # saving models\n    model_checkpoint['G_H_Z']=G_H_Z.state_dict()\n    model_checkpoint['G_Z_H']=G_Z_H.state_dict()\n    model_checkpoint['D_Z']=D_Z.state_dict()\n    model_checkpoint['D_H']=D_H.state_dict()\n    save_checkpoint(model_checkpoint)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-19T08:48:29.109597Z","iopub.execute_input":"2022-02-19T08:48:29.109919Z","iopub.status.idle":"2022-02-19T09:50:21.876477Z","shell.execute_reply.started":"2022-02-19T08:48:29.109863Z","shell.execute_reply":"2022-02-19T09:50:21.874668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./horse_output","metadata":{"execution":{"iopub.status.busy":"2022-02-19T08:44:53.190704Z","iopub.execute_input":"2022-02-19T08:44:53.191102Z","iopub.status.idle":"2022-02-19T08:44:53.989195Z","shell.execute_reply.started":"2022-02-19T08:44:53.191047Z","shell.execute_reply":"2022-02-19T08:44:53.988163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! zip -r zebraoutput.zip zebra_output","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:53:15.993543Z","iopub.execute_input":"2022-02-19T09:53:15.993948Z","iopub.status.idle":"2022-02-19T09:53:20.71992Z","shell.execute_reply.started":"2022-02-19T09:53:15.9939Z","shell.execute_reply":"2022-02-19T09:53:20.718746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}